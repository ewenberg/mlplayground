{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X[N,C,H,W]: {}\".format(X.shape))\n",
    "    print(\"Shape of y: {}\".format(y.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying an image with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image = X[1][0]\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying an image with iPython display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACaklEQVR4nFWSu2sUURTGv3tn7p2ZndlHdjLZzCZrJMEQQewFCy1EVOz9BwQbQVCws7MTwdLOQgRtgkWwFCw0jY9CVxI0TzfksXGfs/O4c+daTCJ6qgM/fufwwQcAAAgF8GzlxevWYwAE/wwBAP5UbahudHgfAPT/2Ll3qvNjda355dOeej4L0L8SHl6fjkLKd/jKiTCs1NONm+s0AyhAcOdusRUzsTrb3NUyd9Tqzy+5GclN+wPX7CwmWvKmPB+D67FsbF9Afvu2m5IkSTtteXaq2ywEPaXtTNWP4GWlBWkCYxRxGa+Fw0CC8EtH8GSfOmkBsaAoeNcY9fjU8OBiDokvVisqsabRp0MHYTicLtfY0c8Fw5htjwcRc9pqg+0vaj64ObJyyJDWxBM/DqyqTPje2Jm5e9v6qJdDJ9HY1pYXi9gAyl1dWM1etR3WAQr4Gi19c5GYQrAo8kgKJiQGOkCBU1LH9wG0QbFn2/uOGxKxzGpukpt9gkUHOzzN7H5nKGIVfradsXIOu7/Rqe2ZljYRF6WUJqnuKm56AAXGrQAwi9VAV5pn2VJi4b3kayWAAkXqAJN9k8tCMBwWqglmYosFCqDASCbABGUZ13Rb/9rQ1EdAmXnObskAGIHOItutzwQZ8QAiclP0VAdcgTALfmipFDUo4TJABxJFejB6uqKl9WzOIfvrHtpZexLQAaUVD8G4TGGT7aAgeGZis9HKc+4S4yeYKbNwYHgHm1lslNBmbCE3VZTgwaMGTQgR1OIVLEMqpgM6UKpYDbxcuuV4/AReGXTlbYpfldJpAAC7cdU87rd/vFw57wN/ADxACG3baN7WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7FAB2FA5EA50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "grayscale_img = np.uint8(X[50][0].numpy() * 255)\n",
    "i = Image.fromarray(grayscale_img, 'L')\n",
    "display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device...\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device...\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute the prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run the train/test cycle and see how it learns over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \n",
      "----------------------------------\n",
      "loss: 2.296443 [    0/60000]\n",
      "loss: 2.292342 [ 6400/60000]\n",
      "loss: 2.279121 [12800/60000]\n",
      "loss: 2.278732 [19200/60000]\n",
      "loss: 2.276874 [25600/60000]\n",
      "loss: 2.251575 [32000/60000]\n",
      "loss: 2.242615 [38400/60000]\n",
      "loss: 2.222303 [44800/60000]\n",
      "loss: 2.232987 [51200/60000]\n",
      "loss: 2.195560 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 0.034725 \n",
      "\n",
      "Epoch 1 \n",
      "----------------------------------\n",
      "loss: 2.213381 [    0/60000]\n",
      "loss: 2.224470 [ 6400/60000]\n",
      "loss: 2.170785 [12800/60000]\n",
      "loss: 2.183640 [19200/60000]\n",
      "loss: 2.179667 [25600/60000]\n",
      "loss: 2.103505 [32000/60000]\n",
      "loss: 2.128769 [38400/60000]\n",
      "loss: 2.074262 [44800/60000]\n",
      "loss: 2.101092 [51200/60000]\n",
      "loss: 2.015541 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.6%, Avg loss: 0.032423 \n",
      "\n",
      "Epoch 2 \n",
      "----------------------------------\n",
      "loss: 2.075792 [    0/60000]\n",
      "loss: 2.104152 [ 6400/60000]\n",
      "loss: 1.985915 [12800/60000]\n",
      "loss: 2.020382 [19200/60000]\n",
      "loss: 2.020219 [25600/60000]\n",
      "loss: 1.879080 [32000/60000]\n",
      "loss: 1.941242 [38400/60000]\n",
      "loss: 1.843256 [44800/60000]\n",
      "loss: 1.911631 [51200/60000]\n",
      "loss: 1.759444 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 0.029267 \n",
      "\n",
      "Epoch 3 \n",
      "----------------------------------\n",
      "loss: 1.885193 [    0/60000]\n",
      "loss: 1.948058 [ 6400/60000]\n",
      "loss: 1.758605 [12800/60000]\n",
      "loss: 1.822750 [19200/60000]\n",
      "loss: 1.864712 [25600/60000]\n",
      "loss: 1.654696 [32000/60000]\n",
      "loss: 1.752715 [38400/60000]\n",
      "loss: 1.635601 [44800/60000]\n",
      "loss: 1.750992 [51200/60000]\n",
      "loss: 1.549872 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 0.026842 \n",
      "\n",
      "Epoch 4 \n",
      "----------------------------------\n",
      "loss: 1.731853 [    0/60000]\n",
      "loss: 1.833999 [ 6400/60000]\n",
      "loss: 1.600180 [12800/60000]\n",
      "loss: 1.685644 [19200/60000]\n",
      "loss: 1.771394 [25600/60000]\n",
      "loss: 1.515523 [32000/60000]\n",
      "loss: 1.625212 [38400/60000]\n",
      "loss: 1.508091 [44800/60000]\n",
      "loss: 1.650248 [51200/60000]\n",
      "loss: 1.425602 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 0.025300 \n",
      "\n",
      "Epoch 5 \n",
      "----------------------------------\n",
      "loss: 1.629279 [    0/60000]\n",
      "loss: 1.757905 [ 6400/60000]\n",
      "loss: 1.493955 [12800/60000]\n",
      "loss: 1.598170 [19200/60000]\n",
      "loss: 1.699090 [25600/60000]\n",
      "loss: 1.423377 [32000/60000]\n",
      "loss: 1.538108 [38400/60000]\n",
      "loss: 1.418378 [44800/60000]\n",
      "loss: 1.577699 [51200/60000]\n",
      "loss: 1.344506 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.024173 \n",
      "\n",
      "Epoch 6 \n",
      "----------------------------------\n",
      "loss: 1.554402 [    0/60000]\n",
      "loss: 1.700601 [ 6400/60000]\n",
      "loss: 1.413131 [12800/60000]\n",
      "loss: 1.533527 [19200/60000]\n",
      "loss: 1.641232 [25600/60000]\n",
      "loss: 1.355240 [32000/60000]\n",
      "loss: 1.474239 [38400/60000]\n",
      "loss: 1.348646 [44800/60000]\n",
      "loss: 1.522269 [51200/60000]\n",
      "loss: 1.285749 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.3%, Avg loss: 0.023292 \n",
      "\n",
      "Epoch 7 \n",
      "----------------------------------\n",
      "loss: 1.495544 [    0/60000]\n",
      "loss: 1.651274 [ 6400/60000]\n",
      "loss: 1.347167 [12800/60000]\n",
      "loss: 1.481730 [19200/60000]\n",
      "loss: 1.595683 [25600/60000]\n",
      "loss: 1.300663 [32000/60000]\n",
      "loss: 1.427182 [38400/60000]\n",
      "loss: 1.293242 [44800/60000]\n",
      "loss: 1.478115 [51200/60000]\n",
      "loss: 1.238497 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 0.022601 \n",
      "\n",
      "Epoch 8 \n",
      "----------------------------------\n",
      "loss: 1.448959 [    0/60000]\n",
      "loss: 1.611254 [ 6400/60000]\n",
      "loss: 1.294146 [12800/60000]\n",
      "loss: 1.436920 [19200/60000]\n",
      "loss: 1.561618 [25600/60000]\n",
      "loss: 1.256242 [32000/60000]\n",
      "loss: 1.392098 [38400/60000]\n",
      "loss: 1.249948 [44800/60000]\n",
      "loss: 1.442544 [51200/60000]\n",
      "loss: 1.201983 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 0.022065 \n",
      "\n",
      "Epoch 9 \n",
      "----------------------------------\n",
      "loss: 1.411185 [    0/60000]\n",
      "loss: 1.581052 [ 6400/60000]\n",
      "loss: 1.252118 [12800/60000]\n",
      "loss: 1.399053 [19200/60000]\n",
      "loss: 1.535427 [25600/60000]\n",
      "loss: 1.220919 [32000/60000]\n",
      "loss: 1.364938 [38400/60000]\n",
      "loss: 1.216134 [44800/60000]\n",
      "loss: 1.413984 [51200/60000]\n",
      "loss: 1.174065 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.021645 \n",
      "\n",
      "Epoch 10 \n",
      "----------------------------------\n",
      "loss: 1.379417 [    0/60000]\n",
      "loss: 1.558228 [ 6400/60000]\n",
      "loss: 1.218850 [12800/60000]\n",
      "loss: 1.368353 [19200/60000]\n",
      "loss: 1.515838 [25600/60000]\n",
      "loss: 1.192646 [32000/60000]\n",
      "loss: 1.342434 [38400/60000]\n",
      "loss: 1.189288 [44800/60000]\n",
      "loss: 1.390808 [51200/60000]\n",
      "loss: 1.152573 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 0.021307 \n",
      "\n",
      "Epoch 11 \n",
      "----------------------------------\n",
      "loss: 1.353061 [    0/60000]\n",
      "loss: 1.539493 [ 6400/60000]\n",
      "loss: 1.192850 [12800/60000]\n",
      "loss: 1.343053 [19200/60000]\n",
      "loss: 1.500728 [25600/60000]\n",
      "loss: 1.170230 [32000/60000]\n",
      "loss: 1.323064 [38400/60000]\n",
      "loss: 1.167766 [44800/60000]\n",
      "loss: 1.371533 [51200/60000]\n",
      "loss: 1.134874 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.021025 \n",
      "\n",
      "Epoch 12 \n",
      "----------------------------------\n",
      "loss: 1.330942 [    0/60000]\n",
      "loss: 1.523503 [ 6400/60000]\n",
      "loss: 1.171406 [12800/60000]\n",
      "loss: 1.322105 [19200/60000]\n",
      "loss: 1.488031 [25600/60000]\n",
      "loss: 1.151242 [32000/60000]\n",
      "loss: 1.306079 [38400/60000]\n",
      "loss: 1.149082 [44800/60000]\n",
      "loss: 1.355326 [51200/60000]\n",
      "loss: 1.119936 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 0.020784 \n",
      "\n",
      "Epoch 13 \n",
      "----------------------------------\n",
      "loss: 1.311468 [    0/60000]\n",
      "loss: 1.509106 [ 6400/60000]\n",
      "loss: 1.153355 [12800/60000]\n",
      "loss: 1.304880 [19200/60000]\n",
      "loss: 1.476810 [25600/60000]\n",
      "loss: 1.135395 [32000/60000]\n",
      "loss: 1.290906 [38400/60000]\n",
      "loss: 1.133513 [44800/60000]\n",
      "loss: 1.341560 [51200/60000]\n",
      "loss: 1.106402 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.020572 \n",
      "\n",
      "Epoch 14 \n",
      "----------------------------------\n",
      "loss: 1.294103 [    0/60000]\n",
      "loss: 1.496184 [ 6400/60000]\n",
      "loss: 1.138093 [12800/60000]\n",
      "loss: 1.290035 [19200/60000]\n",
      "loss: 1.466960 [25600/60000]\n",
      "loss: 1.121362 [32000/60000]\n",
      "loss: 1.276856 [38400/60000]\n",
      "loss: 1.120485 [44800/60000]\n",
      "loss: 1.330196 [51200/60000]\n",
      "loss: 1.094105 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.020381 \n",
      "\n",
      "Epoch 15 \n",
      "----------------------------------\n",
      "loss: 1.278595 [    0/60000]\n",
      "loss: 1.484337 [ 6400/60000]\n",
      "loss: 1.124991 [12800/60000]\n",
      "loss: 1.277379 [19200/60000]\n",
      "loss: 1.457997 [25600/60000]\n",
      "loss: 1.109923 [32000/60000]\n",
      "loss: 1.264111 [38400/60000]\n",
      "loss: 1.109099 [44800/60000]\n",
      "loss: 1.320434 [51200/60000]\n",
      "loss: 1.082591 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.020207 \n",
      "\n",
      "Epoch 16 \n",
      "----------------------------------\n",
      "loss: 1.264768 [    0/60000]\n",
      "loss: 1.472840 [ 6400/60000]\n",
      "loss: 1.113648 [12800/60000]\n",
      "loss: 1.266239 [19200/60000]\n",
      "loss: 1.450491 [25600/60000]\n",
      "loss: 1.099423 [32000/60000]\n",
      "loss: 1.252313 [38400/60000]\n",
      "loss: 1.099329 [44800/60000]\n",
      "loss: 1.312590 [51200/60000]\n",
      "loss: 1.071128 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.020047 \n",
      "\n",
      "Epoch 17 \n",
      "----------------------------------\n",
      "loss: 1.252163 [    0/60000]\n",
      "loss: 1.461615 [ 6400/60000]\n",
      "loss: 1.103332 [12800/60000]\n",
      "loss: 1.256225 [19200/60000]\n",
      "loss: 1.444090 [25600/60000]\n",
      "loss: 1.090144 [32000/60000]\n",
      "loss: 1.240822 [38400/60000]\n",
      "loss: 1.091082 [44800/60000]\n",
      "loss: 1.305768 [51200/60000]\n",
      "loss: 1.060312 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.019898 \n",
      "\n",
      "Epoch 18 \n",
      "----------------------------------\n",
      "loss: 1.240677 [    0/60000]\n",
      "loss: 1.450560 [ 6400/60000]\n",
      "loss: 1.094423 [12800/60000]\n",
      "loss: 1.247181 [19200/60000]\n",
      "loss: 1.437575 [25600/60000]\n",
      "loss: 1.081865 [32000/60000]\n",
      "loss: 1.229850 [38400/60000]\n",
      "loss: 1.083670 [44800/60000]\n",
      "loss: 1.300065 [51200/60000]\n",
      "loss: 1.049845 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.019759 \n",
      "\n",
      "Epoch 19 \n",
      "----------------------------------\n",
      "loss: 1.230272 [    0/60000]\n",
      "loss: 1.440064 [ 6400/60000]\n",
      "loss: 1.086264 [12800/60000]\n",
      "loss: 1.238847 [19200/60000]\n",
      "loss: 1.431937 [25600/60000]\n",
      "loss: 1.074580 [32000/60000]\n",
      "loss: 1.219505 [38400/60000]\n",
      "loss: 1.077474 [44800/60000]\n",
      "loss: 1.295261 [51200/60000]\n",
      "loss: 1.039466 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 0.019627 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(\"Epoch {} \\n----------------------------------\".format(t))\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to pytorch_quickstart.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"pytorch_quickstart.pth\")\n",
    "print(\"Saved model to pytorch_quickstart.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load the model and make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"pytorch_quickstart.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements of dataset are of type: <class 'tuple'> and have length: 2\n"
     ]
    }
   ],
   "source": [
    "sample = test_data[0]\n",
    "print(\"Elements of dataset are of type: {} and have length: {}\".format(type(sample), len(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000, 0.0275,\n",
       "          0.0000, 0.1451, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000,\n",
       "          0.1059, 0.3294, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.4667, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
       "          0.3451, 0.5608, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
       "          0.3647, 0.4157, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.2078,\n",
       "          0.5059, 0.4706, 0.5765, 0.6863, 0.6157, 0.6510, 0.5294, 0.6039,\n",
       "          0.6588, 0.5490, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0431, 0.5373,\n",
       "          0.5098, 0.5020, 0.6275, 0.6902, 0.6235, 0.6549, 0.6980, 0.5843,\n",
       "          0.5922, 0.5647, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
       "          0.0078, 0.0039, 0.0000, 0.0118, 0.0000, 0.0000, 0.4510, 0.4471,\n",
       "          0.4157, 0.5373, 0.6588, 0.6000, 0.6118, 0.6471, 0.6549, 0.5608,\n",
       "          0.6157, 0.6196, 0.0431, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.3490, 0.5451, 0.3529,\n",
       "          0.3686, 0.6000, 0.5843, 0.5137, 0.5922, 0.6627, 0.6745, 0.5608,\n",
       "          0.6235, 0.6627, 0.1882, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157,\n",
       "          0.0039, 0.0000, 0.0000, 0.0000, 0.3843, 0.5333, 0.4314, 0.4275,\n",
       "          0.4314, 0.6353, 0.5294, 0.5647, 0.5843, 0.6235, 0.6549, 0.5647,\n",
       "          0.6196, 0.6627, 0.4667, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1020, 0.4235, 0.4588, 0.3882, 0.4353, 0.4588,\n",
       "          0.5333, 0.6118, 0.5255, 0.6039, 0.6039, 0.6118, 0.6275, 0.5529,\n",
       "          0.5765, 0.6118, 0.6980, 0.0000],\n",
       "         [0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
       "          0.2078, 0.3608, 0.4588, 0.4353, 0.4039, 0.4510, 0.5059, 0.5255,\n",
       "          0.5608, 0.6039, 0.6471, 0.6667, 0.6039, 0.5922, 0.6039, 0.5608,\n",
       "          0.5412, 0.5882, 0.6471, 0.1686],\n",
       "         [0.0000, 0.0000, 0.0902, 0.2118, 0.2549, 0.2980, 0.3333, 0.4627,\n",
       "          0.5020, 0.4824, 0.4353, 0.4431, 0.4627, 0.4980, 0.4902, 0.5451,\n",
       "          0.5216, 0.5333, 0.6275, 0.5490, 0.6078, 0.6314, 0.5647, 0.6078,\n",
       "          0.6745, 0.6314, 0.7412, 0.2431],\n",
       "         [0.0000, 0.2667, 0.3686, 0.3529, 0.4353, 0.4471, 0.4353, 0.4471,\n",
       "          0.4510, 0.4980, 0.5294, 0.5333, 0.5608, 0.4941, 0.4980, 0.5922,\n",
       "          0.6039, 0.5608, 0.5804, 0.4902, 0.6353, 0.6353, 0.5647, 0.5412,\n",
       "          0.6000, 0.6353, 0.7686, 0.2275],\n",
       "         [0.2745, 0.6627, 0.5059, 0.4078, 0.3843, 0.3922, 0.3686, 0.3804,\n",
       "          0.3843, 0.4000, 0.4235, 0.4157, 0.4667, 0.4706, 0.5059, 0.5843,\n",
       "          0.6118, 0.6549, 0.7451, 0.7451, 0.7686, 0.7765, 0.7765, 0.7333,\n",
       "          0.7725, 0.7412, 0.7216, 0.1412],\n",
       "         [0.0627, 0.4941, 0.6706, 0.7373, 0.7373, 0.7216, 0.6706, 0.6000,\n",
       "          0.5294, 0.4706, 0.4941, 0.4980, 0.5725, 0.7255, 0.7647, 0.8196,\n",
       "          0.8157, 1.0000, 0.8196, 0.6941, 0.9608, 0.9882, 0.9843, 0.9843,\n",
       "          0.9686, 0.8627, 0.8078, 0.1922],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0471, 0.2627, 0.4157, 0.6431, 0.7255,\n",
       "          0.7804, 0.8235, 0.8275, 0.8235, 0.8157, 0.7451, 0.5882, 0.3216,\n",
       "          0.0314, 0.0000, 0.0000, 0.0000, 0.6980, 0.8157, 0.7373, 0.6863,\n",
       "          0.6353, 0.6196, 0.5922, 0.0431],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
